{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f637fa2",
   "metadata": {},
   "source": [
    "## Here we will use furher improvements of previously zero-shot approach.\n",
    "\n",
    "Changes are:\n",
    "- Ask model choose one or few option for classification of lyrics. Previous model generated too much FP.\n",
    "- Add few shot examples of classification\n",
    "- Add reasoning fiels into json\n",
    "\n",
    "I have used [chat gpt](https://chatgpt.com/g/g-6769db0aa91c8191bf46eeac95f5e055-system-prompt-generator-for-reasoning-models) System Prompt Generator for prompt improvement. Link to dialog: https://chatgpt.com/share/6851f905-1230-8012-90c5-534a8f75a958\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c89bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e9aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 03:08:29,441 - numexpr.utils - INFO - Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2025-06-18 03:08:29,442 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from src.utils import logger, DatasetTypes\n",
    "from src.data import get_datasets, get_dataloaders\n",
    "from src.metrics import GenrePredictorInterface, evaluate_model\n",
    "from src.model import get_pretrained\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "import random\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd3494",
   "metadata": {},
   "source": [
    "## Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98cc299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer, model = get_pretrained(model_name, device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d5c27",
   "metadata": {},
   "source": [
    "## Get dataset with all genres and 1,294,054 examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a459fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = '../data/all_genres_downsampled.csv'\n",
    "dataset_type = DatasetTypes.hundred\n",
    "batch_size = 8\n",
    "\n",
    "data_dict = get_datasets(path_to_csv, tokenizer, dataset_type=dataset_type)\n",
    "train_dataset, val_dataset, test_dataset = data_dict['train_dataset'], data_dict['val_dataset'], data_dict['test_dataset']\n",
    "\n",
    "idx2genre, genre2idx = data_dict['idx2genre'], data_dict['genre2idx']\n",
    "genres = [key for key, _ in genre2idx.items()]\n",
    "\n",
    "traid_loader, val_loader, test_loader = get_dataloaders(train_dataset, val_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78edb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''You are a music genre classification expert. Your task is to analyze song lyrics and decide which single music genre from the given list they most likely belong to. You must provide a short explanation for your choice in the \"reasoning\" field and then output the selected genre using the \"predict\" field.\n",
    "\n",
    "**Available genres:**\n",
    "<genres>\n",
    "\n",
    "**Input format:**\n",
    "```json\n",
    "{\n",
    "    \"lyrics\": \"Text of the song lyrics\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Output format:**\n",
    "```json\n",
    "{\n",
    "    \"reasoning\": \"Explain briefly why the lyrics fit the selected genre\",\n",
    "    \"predict\": \"genre_name\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Few-shot examples:**\n",
    "\n",
    "Example 1:\n",
    "```json\n",
    "Input:\n",
    "{\n",
    "    \"lyrics\": \"I got my hands up, they're playing my song, I know I'm gonna be OK...\"\n",
    "}\n",
    "Output:\n",
    "{\n",
    "    \"reasoning\": \"The lyrics talk about dancing, feeling good, and have a carefree theme typical of pop songs.\",\n",
    "    \"predict\": \"pop\"\n",
    "}\n",
    "```\n",
    "\n",
    "Example 2:\n",
    "```json\n",
    "Input:\n",
    "{\n",
    "    \"lyrics\": \"Gunshots echo through the night, I’ve seen too many die young...\"\n",
    "}\n",
    "Output:\n",
    "{\n",
    "    \"reasoning\": \"The lyrics are gritty and socially conscious, with a storytelling style that is common in hip hop.\",\n",
    "    \"predict\": \"hip-hop\"\n",
    "}\n",
    "```\n",
    "\n",
    "Example 3:\n",
    "```json\n",
    "Input:\n",
    "{\n",
    "    \"lyrics\": \"Rolling down that old dirt road, truck tires kickin' dust in the air...\"\n",
    "}\n",
    "Output:\n",
    "{\n",
    "    \"reasoning\": \"Mentions of trucks, dirt roads, and rural imagery are strong indicators of country music.\",\n",
    "    \"predict\": \"country\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Lyrics for classification:**\n",
    "```json\n",
    "{\n",
    "    \"lyrics\": \"%s\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Your output:**\n",
    "'''\n",
    "\n",
    "PROMPT = PROMPT.replace('<genres>', str(genres))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c06c4",
   "metadata": {},
   "source": [
    "## Main mechanic and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7123789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "def get_input_text(lyrics, enable_thinking=False):\n",
    "    instruct = PROMPT % lyrics\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruct}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    return text\n",
    "\n",
    "def parse_model_response(generated_ids, model_inputs_len):\n",
    "    assert generated_ids.ndim == 1\n",
    "    \n",
    "    output_ids = generated_ids[model_inputs_len:].tolist() \n",
    "    try:\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "        \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    assert len(content) != 0, 'Error. Output content is empty!'\n",
    "    return thinking_content, content\n",
    "\n",
    "def parse_output_json(response: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Parses the model output to extract the 'reasoning' and 'predict' fields.\n",
    "\n",
    "    Args:\n",
    "        response (str): Raw text response from the model.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing (reasoning, predicted_genre).\n",
    "                         Returns (\"\", \"\") if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Match a JSON object containing both \"reasoning\" and \"predict\"\n",
    "        match = re.search(r'\\{[^}]*\"reasoning\"\\s*:\\s*\"[^\"]*\",\\s*\"predict\"\\s*:\\s*\"[^\"]*\"\\s*\\}', response, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(0)\n",
    "            data = json.loads(json_str)\n",
    "            reasoning = data.get(\"reasoning\", \"\").strip()\n",
    "            genre = data.get(\"predict\", \"\").strip()\n",
    "            return reasoning, genre\n",
    "    except Exception as e:\n",
    "        print(f\"Parse error: {e}\")\n",
    "    return \"\", \"\"  # fallback if parsing fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ec4dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "You are a music genre classification expert. Your task is to analyze song lyrics and decide which single music genre from the given list they most likely belong to. You must provide a short explanation for your choice in the \"reasoning\" field and then output the selected genre using the \"predict\" field.\n",
      "\n",
      "**Available genres:**\n",
      "['alt-country', 'alt-rock', 'alternative', 'ambient', 'axé', 'black-metal', 'blues', 'bossa-nova', 'chillwave', 'classic-rock', 'classical', 'cloud-rap', 'country', 'dance', 'dancehall', 'death-metal', 'deathcore', 'disco', 'doom-metal', 'dream-pop', 'drum&bass', 'dub', 'electro-pop', 'electronic', 'electronica', 'emo', 'emo-rap', 'folk', 'forró', 'funk', 'funk-carioca', 'garage-rock', 'gothic', 'grunge', 'hard-rock', 'hardcore', 'heavy-metal', 'hip-hop', 'house', 'indie', 'indie-pop', 'indie-rock', 'j-pop', 'j-rock', 'jazz', 'jovem-guarda', 'k-pop', 'math-rock', 'melodic-death-metal', 'metal', 'metalcore', 'mpb', 'new-wave', 'pagode', 'pop', 'pop-punk', 'pop-rock', 'post-hardcore', 'post-punk', 'power-metal', 'power-pop', 'progressive-metal', 'progressive-rock', 'psychedelic', 'psychedelic-rock', 'punk', 'punk-rock', 'r&b', 'rap', 'reggae', 'reggaeton', 'religion', 'rock', 'rockabilly', 'samba', 'screamo', 'sertanejo', 'shoegaze', 'soft-rock', 'soul', 'swing', 'synth-pop', 'techno', 'thrash-metal', 'trance', 'trap', 'trip-hop', 'tropical-house']\n",
      "\n",
      "**Input format:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"Text of the song lyrics\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Output format:**\n",
      "```json\n",
      "{\n",
      "    \"reasoning\": \"Explain briefly why the lyrics fit the selected genre\",\n",
      "    \"predict\": \"genre_name\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Few-shot examples:**\n",
      "\n",
      "Example 1:\n",
      "```json\n",
      "Input:\n",
      "{\n",
      "    \"lyrics\": \"I got my hands up, they're playing my song, I know I'm gonna be OK...\"\n",
      "}\n",
      "Output:\n",
      "{\n",
      "    \"reasoning\": \"The lyrics talk about dancing, feeling good, and have a carefree theme typical of pop songs.\",\n",
      "    \"predict\": \"pop\"\n",
      "}\n",
      "```\n",
      "\n",
      "Example 2:\n",
      "```json\n",
      "Input:\n",
      "{\n",
      "    \"lyrics\": \"Gunshots echo through the night, I’ve seen too many die young...\"\n",
      "}\n",
      "Output:\n",
      "{\n",
      "    \"reasoning\": \"The lyrics are gritty and socially conscious, with a storytelling style that is common in hip hop.\",\n",
      "    \"predict\": \"hip-hop\"\n",
      "}\n",
      "```\n",
      "\n",
      "Example 3:\n",
      "```json\n",
      "Input:\n",
      "{\n",
      "    \"lyrics\": \"Rolling down that old dirt road, truck tires kickin' dust in the air...\"\n",
      "}\n",
      "Output:\n",
      "{\n",
      "    \"reasoning\": \"Mentions of trucks, dirt roads, and rural imagery are strong indicators of country music.\",\n",
      "    \"predict\": \"country\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Lyrics for classification:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"[Verse 1] Well, I'm standing here, freezing, outside your golden garden Uh got my ladder, leaned up against your wall Tonight's the night we planned to run away together Come on Dolly Mae, there's no time to stall But now you're telling me [Chorus] I think I better wait until tomorrow I think I bett\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output:**\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: ```json\n",
      "{\n",
      "    \"reasoning\": \"The lyrics describe a reflective and introspective tone, with references to a garden, a ladder, and a future plan, which are common in soul music. The mention of 'outside your golden garden' and 'running away together' also aligns with a romantic or emotional theme typical of soul.\",\n",
      "    \"predict\": \"soul\"\n",
      "}\n",
      "```\n",
      "Reasoning: The lyrics describe a reflective and introspective tone, with references to a garden, a ladder, and a future plan, which are common in soul music. The mention of 'outside your golden garden' and 'running away together' also aligns with a romantic or emotional theme typical of soul.\n",
      "Predicted genre: soul\n",
      "Ground truth genre: jazz,pop,rock\n",
      "Genre idx: 79\n"
     ]
    }
   ],
   "source": [
    "lyrics = val_dataset[0]['features']['lyrics']\n",
    "genre = val_dataset[0]['features']['genre']\n",
    "\n",
    "truncated = lyrics[:300]\n",
    "\n",
    "input_text = get_input_text(truncated, enable_thinking=False,)\n",
    "print(input_text)\n",
    "\n",
    "model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1337,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "thinking_content, content = parse_model_response(generated_ids[0], len(model_inputs.input_ids[0]))\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n",
    "\n",
    "reasoning, predicted_genre = parse_output_json(content)\n",
    "print(f'Reasoning: {reasoning}')\n",
    "print(f'Predicted genre: {predicted_genre}')\n",
    "print(f'Ground truth genre: {genre}')\n",
    "print(f'Genre idx: {genre2idx[predicted_genre]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91a81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccess_sample(sample: str, truncation_len: int = 300, max_new_tokens: int = 1337, enable_thinking=False) -> np.array:\n",
    "    ''' Make predictions for one sample: whether it belongs to each genre. \n",
    "        Returns np array with 1 in corresponding places if lyrics belongs to genre.'''\n",
    "\n",
    "    truncated = sample[:truncation_len]\n",
    "    \n",
    "    input_text = get_input_text(truncated, enable_thinking=enable_thinking)\n",
    "    \n",
    "    model_inputs = tokenizer([input_text], return_tensors=\"pt\", padding=True, padding_side='left').to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    \n",
    "    preds = np.zeros(len(genres), dtype=np.int32)\n",
    "    thinking_content, content = parse_model_response(generated_ids[0], len(model_inputs['input_ids'][0]))\n",
    "    reasoning, genre = parse_output_json(content)\n",
    "    preds[genre2idx[genre]] = 1\n",
    "\n",
    "    return preds\n",
    "\n",
    "def get_input_texts(batch: str, enable_thinking=False, truncation_len=300):\n",
    "    input_texts = []\n",
    "    \n",
    "    for lyrics in batch:\n",
    "        input_text = get_input_text(lyrics[:truncation_len], enable_thinking=enable_thinking)\n",
    "        input_texts.append(input_text) \n",
    "    \n",
    "    return input_texts\n",
    "\n",
    "def proccess_batch(batch: List[str], truncation_len: int = 300, max_new_tokens: int = 1337, enable_thinking=False) -> np.array:\n",
    "    input_texts = get_input_texts(batch, enable_thinking=enable_thinking, truncation_len=truncation_len)\n",
    "    \n",
    "    model_inputs_batch = tokenizer(input_texts, return_tensors=\"pt\", padding=True, padding_side='left').to(model.device)\n",
    "    \n",
    "    generated_ids_batch = model.generate(\n",
    "        **model_inputs_batch,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    \n",
    "    list_preds = []\n",
    "    for i, (generated_ids, model_inputs) in enumerate(zip(generated_ids_batch, model_inputs_batch['input_ids'])):\n",
    "        preds = np.zeros(len(genres), dtype=np.int32)\n",
    "        thinking_content, content = parse_model_response(generated_ids, len(model_inputs))\n",
    "        reasoning, genre = parse_output_json(content)\n",
    "        genre_idx = genre2idx.get(genre, random.randint(0, len(genres)))\n",
    "        preds[genre_idx] = 1\n",
    "        list_preds.append(preds)\n",
    "\n",
    "    return np.stack(list_preds)\n",
    "\n",
    "\n",
    "def one_hot_encoded_to_genre_list(predictions, idx2genre: dict = None):\n",
    "    ''' Predictions is array on n_genres size, where 1 if lyrics belongs to that genre and 0 if not'''    \n",
    "    genre_list = []\n",
    "    for i, value in enumerate(predictions):\n",
    "        if value == 1:\n",
    "            genre_list.append(idx2genre[i])\n",
    "    \n",
    "    return genre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e5f740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A relapse of my body Sends my mind into multiple seizures Psychologically a new human being One that has never been Cursed by the shamen his voodoo spell has my soul My limbs go numb I can't control my own thought Are his now his evil consuming me ever telling me begin the clit carving Slowly turning me, into a flesh eating zombie Knowing this spell can only be broken by the vaginal skins of young women I proceed to find the meat their bleeding cunts will set me free Warmth seeping from this Body Rotted After I sucked the blood from her ass I feel more alive more alive than I've ever been Even though now I'm dead within My mouth drools As I slice your perinium My body smeared With the guts I've extracted through her hole, came swollen organs cunnilingus with the mutilated My spirit returned from the dead Released by the priest but I felt more real when I was dead The curse is broken I have a dependence on vaginal skin It's become my sexual addiction I must slit, the twitching clit Rotted cavity hold the juice Between the legs, I love to carve My cock is dripping with her blood\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "['psychedelic']\n"
     ]
    }
   ],
   "source": [
    "sample = val_dataset[1]['features']['lyrics']\n",
    "print(sample)\n",
    "\n",
    "preds = proccess_sample(sample)\n",
    "print(preds)\n",
    "\n",
    "genre_list = one_hot_encoded_to_genre_list(preds, idx2genre)\n",
    "print(genre_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec8c59",
   "metadata": {},
   "source": [
    "## Metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36688927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for evaluation through dataset\n",
    "\n",
    "class ZeroShotClassifier(GenrePredictorInterface):\n",
    "    def predict(self, batch_features: dict) -> np.array:\n",
    "        lyrics_list = []\n",
    "        \n",
    "        for features in batch_features['features']:\n",
    "            lyrics = features['lyrics']\n",
    "            lyrics_list.append(lyrics)\n",
    "\n",
    "        return proccess_batch(lyrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c37a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_classifier = ZeroShotClassifier()\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "result = zero_shot_classifier.predict(batch)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7760005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [01:05<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.014204545454545454\n",
      "Recall: 0.008116883116883118\n",
      "F1-score: 0.007843521421107629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(zero_shot_classifier, test_loader)\n",
    "\n",
    "print(\"Precision:\", metrics['precision'])\n",
    "print(\"Recall:\", metrics['recall'])\n",
    "print(\"F1-score:\", metrics['f1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
