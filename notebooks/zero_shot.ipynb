{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f637fa2",
   "metadata": {},
   "source": [
    "## Here we will use zero-shot learning method as baseline for genre classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c89bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e9aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 22:51:43,976 - numexpr.utils - INFO - Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2025-05-27 22:51:43,977 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from src.utils import logger, DatasetTypes\n",
    "from src.data import get_datasets, get_dataloaders, one_hot_encoded_to_genre_list\n",
    "from src.metrics import GenrePredictorInterface, evaluate_model\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd3494",
   "metadata": {},
   "source": [
    "## Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98cc299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simpliest model for demonstration scenario\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d5c27",
   "metadata": {},
   "source": [
    "## Get dataset with all genres and 1,294,054 examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a459fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = '../data/all_genres_downsampled.csv'  # Can be obtained by pain and prepare data notebook\n",
    "data_dict = get_datasets(path_to_csv, tokenizer, dataset_type=DatasetTypes.small)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = data_dict['train_dataset'], data_dict['val_dataset'], data_dict['test_dataset']\n",
    "idx2genre, genre2idx = data_dict['idx2genre'], data_dict['genre2idx']\n",
    "genres = [key for key, _ in genre2idx.items()]\n",
    "\n",
    "batch_size = 16\n",
    "traid_loader, val_loader, test_loader = get_dataloaders(train_dataset, val_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814c30d",
   "metadata": {},
   "source": [
    "## Sobstvenno, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f44f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v1 = '''You are a music genre expert. You will determine whether a song belongs to a specific genre based on its lyrics. You will be provided with a JSON input containing the lyrics and the target genre. Respond with 1 if the song likely belongs to the specified genre, and 0 if it does not.\n",
    "\n",
    "**Input format:**\n",
    "```json\n",
    "{\n",
    "    \"lyrics\": \"Lyrics of the song\",\n",
    "    \"genre\": \"Target genre\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Output format:**\n",
    "```json\n",
    "{\n",
    "    \"predict\": 1  // if the song belongs to the genre\n",
    "    // or\n",
    "    \"predict\": 0  // if it does not\n",
    "}\n",
    "```\n",
    "\n",
    "**Lyrics with genre for classification:**\n",
    "```json\n",
    "{\n",
    "    \"lyrics\": \"%s\",\n",
    "    \"genre\": \"%s\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Your output**:\n",
    "'''\n",
    "\n",
    "def parse_model_response(response: str) -> int:\n",
    "    try:\n",
    "        # Попробуем извлечь JSON через регулярку (на случай мусора вокруг)\n",
    "        match = re.search(r'\\{[^}]*\"predict\"\\s*:\\s*(0|1)[^}]*\\}', response)\n",
    "        if match:\n",
    "            data = json.loads(match.group(0))\n",
    "            return int(data['predict'])\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "\n",
    "    raise ValueError(\"Could not parse prediction from model response.\")\n",
    "\n",
    "\n",
    "class ZeroShotClassifier(GenrePredictorInterface):\n",
    "    def __init__(self, model, tokenizer, genres, prompt_template, device=\"cuda\", max_lyrics_length=300):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.genres = genres  # список всех возможных жанров\n",
    "        self.device = device\n",
    "        self.max_lyrics_length = max_lyrics_length\n",
    "        self.prompt_template = prompt_template\n",
    "        \n",
    "    def _make_prompts(self, lyrics: str) -> list[str]:\n",
    "        truncated = lyrics[:self.max_lyrics_length].replace('\\n', ' ').replace('\"', \"'\")\n",
    "        prompts = [self.prompt_template % (truncated, genre) for genre in self.genres]\n",
    "        return prompts\n",
    "\n",
    "    def _parse_response(self, response: str) -> int:\n",
    "        try:\n",
    "            match = re.search(r'\\{[^}]*\"predict\"\\s*:\\s*(0|1)[^}]*\\}', response)\n",
    "            if match:\n",
    "                data = json.loads(match.group(0))\n",
    "                return int(data[\"predict\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\")\n",
    "        return 0  # fallback to 0 if anything goes wrong\n",
    "    \n",
    "\n",
    "def make_prompts(lyrics: str, genres) -> list[str]:\n",
    "    truncated = lyrics[:300].replace('\\n', ' ').replace('\"', \"'\")\n",
    "    prompts = [prompt_v1 % (truncated, genre) for genre in genres]\n",
    "    return prompts\n",
    "\n",
    "def parse_response(response: str) -> int:\n",
    "    try:\n",
    "        match = re.search(r'\\{[^}]*\"predict\"\\s*:\\s*(0|1)[^}]*\\}', response)\n",
    "        if match:\n",
    "            data = json.loads(match.group(0))\n",
    "            return int(data[\"predict\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Parse error: {e}\")\n",
    "    return 0  # fallback to 0 if anything goes wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13c477",
   "metadata": {},
   "source": [
    "Main mechanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71cb54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1] Well, I'm standing here, freezing, outside your golden garden Uh got my ladder, leaned up \n",
      "jazz\n",
      "You are a music genre expert. You will determine whether a song belongs to a specific genre based on its lyrics. You will be provided with a JSON input containing the lyrics and the target genre. Respond with 1 if the song likely belongs to the specified genre, and 0 if it does not.\n",
      "\n",
      "**Input format:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"Lyrics of the song\",\n",
      "    \"genre\": \"Target genre\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Output format:**\n",
      "```json\n",
      "{\n",
      "    \"predict\": 1  // if the song belongs to the genre\n",
      "    // or\n",
      "    \"predict\": 0  // if it does not\n",
      "}\n",
      "```\n",
      "\n",
      "**Lyrics with genre for classification:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"[Verse 1] Well, I'm standing here, freezing, outside your golden garden Uh got my ladder, leaned up against your wall Tonight's the night we planned to run away together Come on Dolly Mae, there's no time to stall But now you're telling me [Chorus] I think I better wait until tomorrow I think I bett\",\n",
      "    \"genre\": \"jazz\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "\n",
      "thinking content: \n",
      "content: ```json\n",
      "{\n",
      "    \"predict\": 1\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# suppose we have lyrics and target genre\n",
    "lyrics = val_dataset[0]['features']['lyrics']\n",
    "target_genre = val_dataset[0]['features']['genre_list'][0]\n",
    "print(lyrics[:100])\n",
    "print(target_genre)\n",
    "\n",
    "# let's try ask model if that song is belongs to target genre\n",
    "truncated = lyrics[:300]\n",
    "instruct = prompt_v1 % (truncated, target_genre)\n",
    "\n",
    "print(instruct)\n",
    "\n",
    "# prepare the model input\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": instruct}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    "    do_sample=False\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1337\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "try:\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "    \n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec722f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StaticCache\n",
    "\n",
    "def generate_with_prefix_cache_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix_past_key_values: Tuple[Tuple[torch.Tensor, torch.Tensor], ...],\n",
    "    postfix_texts: List[str],\n",
    "    device: str = \"cuda\",\n",
    "    max_new_tokens: int = 128,\n",
    ") -> List[str]:\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = len(postfix_texts)\n",
    "\n",
    "    # 1. Токенизируем все postfix'ы, паддим\n",
    "    inputs = tokenizer(\n",
    "        postfix_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False\n",
    "    ).to(device)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]  # [B, L]\n",
    "\n",
    "    # 2. Клонируем prefix_past_key_values на батч\n",
    "    static_cache = StaticCache.from_legacy_cache(prefix_past_key_values)\n",
    "\n",
    "    # Повторение кеша для каждого элемента в батче\n",
    "    batched_cache = static_cache.expand(batch_size)\n",
    "    \n",
    "    # 3. Подготовка\n",
    "    generated = [[] for _ in range(batch_size)]\n",
    "    is_finished = [False] * batch_size\n",
    "\n",
    "    # 4. Генерация токенов\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=batched_cache,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        logits = outputs.logits[:, -1, :]  # [B, vocab_size]\n",
    "        batched_cache = outputs.past_key_values\n",
    "\n",
    "        next_token_ids = torch.argmax(logits, dim=-1)  # [B]\n",
    "\n",
    "        for i, token_id in enumerate(next_token_ids.tolist()):\n",
    "            if not is_finished[i]:\n",
    "                if token_id == tokenizer.eos_token_id:\n",
    "                    is_finished[i] = True\n",
    "                else:\n",
    "                    generated[i].append(token_id)\n",
    "\n",
    "        if all(is_finished):\n",
    "            break\n",
    "\n",
    "        # Обновляем input_ids: shape [B, 1]\n",
    "        input_ids = next_token_ids.unsqueeze(1)\n",
    "\n",
    "    # 5. Декодирование результатов\n",
    "    return [\n",
    "        tokenizer.decode(g, skip_special_tokens=True)\n",
    "        for g in generated\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f942c9",
   "metadata": {},
   "source": [
    "Try kv-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0099f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_prefix_cache(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix_past_key_values: Tuple[Tuple[torch.Tensor, torch.Tensor], ...],\n",
    "    postfix_text: str,\n",
    "    device: str = \"cuda\",\n",
    "    max_new_tokens: int = 256,\n",
    ") -> Tuple[torch.Tensor, List[int]]:\n",
    "    \"\"\"\n",
    "    Генерирует продолжение для postfix_text, используя заранее посчитанный\n",
    "    past_key_values для общего префикса.\n",
    "\n",
    "    Args:\n",
    "        model: AutoModelForCausalLM с поддержкой use_cache.\n",
    "        tokenizer: соответствующий AutoTokenizer.\n",
    "        prefix_past_key_values: past_key_values, полученный один раз для префикса.\n",
    "        postfix_text: текст, который следует токенизировать и «досчитать».\n",
    "        device: device для тензоров.\n",
    "        max_new_tokens: сколько токенов генерировать.\n",
    "\n",
    "    Returns:\n",
    "        generated_ids: тензор [1, L] с ID сгенерированных токенов.\n",
    "        full_sequence_ids: список всех ID (postfix + сгенерированных).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1) Токенизируем сразу postfix, но без учёта префикса в input_ids\n",
    "    inputs = tokenizer(\n",
    "        postfix_text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False  # важно, чтобы не двойных BOS/EOS\n",
    "    ).to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # 2) Устанавливаем past = префикс‑кеш\n",
    "    past = prefix_past_key_values\n",
    "\n",
    "    generated_ids = []\n",
    "\n",
    "    # 3) Генерируем по одному токену\n",
    "    for _ in range(max_new_tokens):\n",
    "        # forward с use_cache=True и подставленным past\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        # logits последнего шага: [1, 1, vocab_size]\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # обновляем кеш\n",
    "        past = outputs.past_key_values\n",
    "\n",
    "        # выбираем следующий токен режимом greedy\n",
    "        next_token_id = torch.argmax(logits, dim=-1)  # shape [1]\n",
    "\n",
    "        # Опционально можно досрочно выйти, увидев EOS\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        generated_ids.append(next_token_id.item())\n",
    "\n",
    "        # готовим input_ids для следующего шага: только что предсказанный токен\n",
    "        input_ids = next_token_id.unsqueeze(-1)       # shape [1,1]\n",
    "\n",
    "\n",
    "    return tokenizer.decode(generated_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298e5ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "You are a music genre expert. You will determine whether a song belongs to a specific genre based on its lyrics. You will be provided with a JSON input containing the lyrics and the target genre. Respond with 1 if the song likely belongs to the specified genre, and 0 if it does not.\n",
      "\n",
      "**Input format:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"Lyrics of the song\",\n",
      "    \"genre\": \"Target genre\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Output format:**\n",
      "```json\n",
      "{\n",
      "    \"predict\": 1  // if the song belongs to the genre\n",
      "    // or\n",
      "    \"predict\": 0  // if it does not\n",
      "}\n",
      "```\n",
      "\n",
      "**Lyrics with genre for classification:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"[Verse 1] Well, I'm standing here, freezing, outside your golden garden Uh got my ladder, leaned up against your wall Tonight's the night we planned to run away together Come on Dolly Mae, there's no time to stall But now you're telling me [Chorus] I think I better wait until tomorrow I think I bett\",\n",
      "    \"genre\": \"jazz\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"predict\": 1\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "lyrics = val_dataset[0]['features']['lyrics']\n",
    "target_genre = val_dataset[0]['features']['genre_list'][0]\n",
    "\n",
    "truncated = lyrics[:300]\n",
    "instruct = prompt_v1 % (truncated, target_genre)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": instruct}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "messages_with_template = [\n",
    "    {\"role\": \"user\", \"content\": prompt_v1}\n",
    "]\n",
    "text_with_template = tokenizer.apply_chat_template(\n",
    "    messages_with_template,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "text_split = text_with_template.split('%s', 1)\n",
    "split_index = text_with_template.index(\"%s\")\n",
    "prefix = text_split[0]\n",
    "suffix = text[split_index:]\n",
    "\n",
    "prefix_inputs = tokenizer(\n",
    "    prefix,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prefix_outputs = model(\n",
    "        **prefix_inputs,\n",
    "        use_cache=True\n",
    "    )\n",
    "    prefix_past = prefix_outputs.past_key_values\n",
    "\n",
    "generated_text = generate_with_prefix_cache(model, tokenizer, prefix_past, suffix)\n",
    "print(prefix + suffix + generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98bb37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_with_prefix_cache(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix_past_key_values,\n",
    "    postfix_ids: torch.Tensor,           # [B, T]\n",
    "    postfix_mask: torch.Tensor,          # [B, T]\n",
    "    device: str = \"cuda\",\n",
    "    max_new_tokens: int = 256,\n",
    "    debug = False\n",
    ") -> List[str]:\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = postfix_ids.to(device)\n",
    "    postfix_mask = postfix_mask.to(device)\n",
    "    batch_size = input_ids.size(0)\n",
    "\n",
    "    past = prefix_past_key_values\n",
    "    generated_ids = [[] for _ in range(batch_size)]\n",
    "    finished = [False] * batch_size\n",
    "\n",
    "    # Определяем длину префикса (по количеству key/value слоёв и их размерности)\n",
    "    prefix_len = past[0][0].size(-2)  # [num_layers][0=key or 1=value][B, H, prefix_len, D]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for step in range(max_new_tokens):\n",
    "            model_inputs = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"past_key_values\": past,\n",
    "                \"use_cache\": True\n",
    "            }\n",
    "\n",
    "            if step == 0:\n",
    "                # Добавляем attention_mask на первый шаг: [B, prefix_len + postfix_len]\n",
    "                # Префиксная часть (всё единицы, потому что они уже прошли)\n",
    "                prefix_attention_mask = torch.ones((batch_size, prefix_len), dtype=torch.long, device=device)\n",
    "                full_attention_mask = torch.cat([prefix_attention_mask, postfix_mask], dim=1)  # [B, prefix + postfix]\n",
    "                model_inputs[\"attention_mask\"] = full_attention_mask\n",
    "\n",
    "            outputs = model(**model_inputs)\n",
    "            logits = outputs.logits[:, -1, :]                # [B, V]\n",
    "            next_tokens = torch.argmax(logits, dim=-1)       # [B]\n",
    "\n",
    "            del input_ids, past\n",
    "            past = outputs.past_key_values\n",
    "            del outputs\n",
    "            input_ids = next_tokens.unsqueeze(-1)            # [B, 1]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                if not finished[i]:\n",
    "                    token = next_tokens[i].item()\n",
    "                    if token == tokenizer.eos_token_id:\n",
    "                        finished[i] = True\n",
    "                    else:\n",
    "                        generated_ids[i].append(token)\n",
    "\n",
    "            if all(finished):\n",
    "                break\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    return [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f18076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotClassifierV1(ZeroShotClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        genres: List[str],\n",
    "        prompt_template: str,\n",
    "        use_kv_cache: bool = True,\n",
    "        device: str = \"cuda\",\n",
    "        max_lyrics_length: int = 300,\n",
    "        batch_size: int = 2\n",
    "    ):\n",
    "        super().__init__(model, tokenizer, genres, prompt_template, device, max_lyrics_length)\n",
    "        self.use_kv_cache = use_kv_cache\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Prepare the prefix tokens once\n",
    "        prefix_template = prompt_template.split(\"%s\")[0]\n",
    "        tokenized = tokenizer(\n",
    "            prefix_template,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        self.prefix_ids = tokenized[\"input_ids\"].to(self.device)[0]\n",
    "        self.prefix_len = self.prefix_ids.size(0)\n",
    "        logger.info(f\"Initialized classifier with prefix_len={self.prefix_len}\")\n",
    "\n",
    "    def _build_prompts_and_map(\n",
    "        self, lyrics_list: List[str]\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        prompts, idx_map = [], []\n",
    "        for idx, txt in enumerate(lyrics_list):\n",
    "            trunc = txt[: self.max_lyrics_length]\n",
    "            for _g in self.genres:\n",
    "                prompts.append(self.prompt_template % (trunc, _g))\n",
    "                idx_map.append(idx)\n",
    "        return prompts, idx_map\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        batch: Dict,\n",
    "        enable_thinking: bool = False,\n",
    "        debug: bool = False\n",
    "    ) -> Tuple[np.ndarray, List[str], List[str]]:\n",
    "        lyrics = [f[\"lyrics\"] for f in batch[\"features\"]]\n",
    "        all_prompts, idx_map = self._build_prompts_and_map(lyrics)\n",
    "        if debug:\n",
    "            logger.info(f\"Total prompts: {len(all_prompts)}\")\n",
    "\n",
    "        if not all_prompts:\n",
    "            return np.zeros((0, len(self.genres))), [], []\n",
    "\n",
    "        # Apply chat template\n",
    "        full_texts = []\n",
    "        for p in all_prompts:\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": p}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=enable_thinking,\n",
    "                do_sample=False\n",
    "            )\n",
    "            full_texts.append(text)\n",
    "        if debug:\n",
    "            logger.info(f\"Example full_text: {full_texts[0]}\")\n",
    "\n",
    "        # Tokenize batch\n",
    "        tok = self.tokenizer(\n",
    "            full_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False\n",
    "        ).to(self.device)\n",
    "        input_ids, attention_mask = tok.input_ids, tok.attention_mask\n",
    "        if debug:\n",
    "            logger.info(f\"input_ids shape: {input_ids.shape}, attention_mask shape: {attention_mask.shape}\")\n",
    "\n",
    "        generated = []\n",
    "\n",
    "        # With KV cache\n",
    "        if self.use_kv_cache:\n",
    "            for i in range(0, input_ids.size(0), self.batch_size):\n",
    "                batch_ids = input_ids[i : i + self.batch_size]\n",
    "                batch_mask = attention_mask[i : i + self.batch_size]\n",
    "                if debug:\n",
    "                    logger.info(f\"Processing batch {i}..{i+self.batch_size}, batch_size={batch_ids.size(0)}\")\n",
    "\n",
    "                postfix_ids = batch_ids[:, self.prefix_len :]\n",
    "                postfix_mask = batch_mask[:, self.prefix_len :]\n",
    "                if debug:\n",
    "                    sample_tok = self.tokenizer.decode(postfix_ids[0], skip_special_tokens=False)\n",
    "                    logger.info(f\"Postfix sample: {sample_tok}\")\n",
    "\n",
    "                prefix_batch = self.prefix_ids.unsqueeze(0).repeat(postfix_ids.size(0), 1)\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(\n",
    "                        input_ids=prefix_batch,\n",
    "                        use_cache=True\n",
    "                    )\n",
    "                    past = out.past_key_values\n",
    "                    if debug:\n",
    "                        logger.info(\"Obtained past_key_values from prefix run\")\n",
    "\n",
    "                cont = batch_generate_with_prefix_cache(\n",
    "                    self.model,\n",
    "                    self.tokenizer,\n",
    "                    past,\n",
    "                    postfix_ids,\n",
    "                    postfix_mask,\n",
    "                    device=self.device,\n",
    "                    max_new_tokens=256,\n",
    "                    debug=debug\n",
    "                )\n",
    "                generated.extend(cont)\n",
    "                if debug:\n",
    "                    logger.info(f\"Generated {len(cont)} continuations for batch starting at {i}. Generated example: {cont}\")\n",
    "\n",
    "        # Without KV cache\n",
    "        else:\n",
    "            for i in range(0, len(full_texts), self.batch_size):\n",
    "                sub = full_texts[i : i + self.batch_size]\n",
    "                if debug:\n",
    "                    logger.info(f\"Generating batch (no KV) {i}..{i+self.batch_size}\")\n",
    "                inp = self.tokenizer(\n",
    "                    sub,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                ).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    outs = self.model.generate(\n",
    "                        **inp,\n",
    "                        max_new_tokens=1024,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                for j in range(len(sub)):\n",
    "                    start = inp[\"input_ids\"][j].size(0)\n",
    "                    txt = self.tokenizer.decode(outs[j][start:], skip_special_tokens=True).strip()\n",
    "                    generated.append(txt)\n",
    "                if debug:\n",
    "                    logger.info(f\"Generated {len(sub)} outputs (no KV) for batch starting at {i}\")\n",
    "\n",
    "        # Build prediction matrix\n",
    "        B, G = len(lyrics), len(self.genres)\n",
    "        preds = np.zeros((B, G), dtype=np.int32)\n",
    "        for idx, text in enumerate(generated):\n",
    "            i = idx_map[idx]\n",
    "            g = idx % G\n",
    "            try:\n",
    "                preds[i, g] = self._parse_response(text)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Parse failed on '{text}': {e}\")\n",
    "\n",
    "        if debug:\n",
    "            logger.info(f\"Final predictions matrix shape: {preds.shape}\")\n",
    "        return preds, generated, full_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49fcbf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 00:27:21,357 - src.utils - INFO - Initialized classifier with prefix_len=157\n",
      "2025-05-28 00:27:21,379 - src.utils - INFO - Total prompts: 1408\n",
      "2025-05-28 00:27:21,452 - src.utils - INFO - Example full_text: <|im_start|>user\n",
      "You are a music genre expert. You will determine whether a song belongs to a specific genre based on its lyrics. You will be provided with a JSON input containing the lyrics and the target genre. Respond with 1 if the song likely belongs to the specified genre, and 0 if it does not.\n",
      "\n",
      "**Input format:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"Lyrics of the song\",\n",
      "    \"genre\": \"Target genre\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Output format:**\n",
      "```json\n",
      "{\n",
      "    \"predict\": 1  // if the song belongs to the genre\n",
      "    // or\n",
      "    \"predict\": 0  // if it does not\n",
      "}\n",
      "```\n",
      "\n",
      "**Lyrics with genre for classification:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"alt-country\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "2025-05-28 00:27:21,645 - src.utils - INFO - input_ids shape: torch.Size([1408, 290]), attention_mask shape: torch.Size([1408, 290])\n",
      "2025-05-28 00:27:21,646 - src.utils - INFO - Processing batch 0..2, batch_size=2\n",
      "2025-05-28 00:27:21,647 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"alt-country\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:21,683 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:25,542 - src.utils - INFO - Generated 2 continuations for batch starting at 0. Generated example:  the lyrics with genre for classification: \n",
      "\n",
      "```js\n",
      "2025-05-28 00:27:25,543 - src.utils - INFO - Processing batch 2..4, batch_size=2\n",
      "2025-05-28 00:27:25,544 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"alternative\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:25,586 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:26,562 - src.utils - INFO - Generated 2 continuations for batch starting at 2. Generated example:  the genre, you can say that.\n",
      "2025-05-28 00:27:26,563 - src.utils - INFO - Processing batch 4..6, batch_size=2\n",
      "2025-05-28 00:27:26,564 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"axé\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:26,606 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:27,983 - src.utils - INFO - Generated 2 continuations for batch starting at 4. Generated example:  the lyrics with genre for classification, you nee\n",
      "2025-05-28 00:27:27,984 - src.utils - INFO - Processing batch 6..8, batch_size=2\n",
      "2025-05-28 00:27:27,985 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"blues\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:28,027 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:31,321 - src.utils - INFO - Generated 2 continuations for batch starting at 6. Generated example:  blues?\n",
      "2025-05-28 00:27:31,321 - src.utils - INFO - Processing batch 8..10, batch_size=2\n",
      "2025-05-28 00:27:31,322 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"chillwave\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:31,357 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:35,469 - src.utils - INFO - Generated 2 continuations for batch starting at 8. Generated example:  the lyrics, I'll tell you whether the song likely\n",
      "2025-05-28 00:27:35,471 - src.utils - INFO - Processing batch 10..12, batch_size=2\n",
      "2025-05-28 00:27:35,472 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"classical\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:35,519 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:40,092 - src.utils - INFO - Generated 2 continuations for batch starting at 10. Generated example:  input format: \n",
      "\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"rics\": \"\n",
      "2025-05-28 00:27:40,092 - src.utils - INFO - Processing batch 12..14, batch_size=2\n",
      "2025-05-28 00:27:40,093 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"country\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:40,134 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:43,254 - src.utils - INFO - Generated 2 continuations for batch starting at 12. Generated example:  {\n",
      " lyrics: \"rics\": \"I scandalized my brother Whil\n",
      "2025-05-28 00:27:43,255 - src.utils - INFO - Processing batch 14..16, batch_size=2\n",
      "2025-05-28 00:27:43,256 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"dancehall\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:43,293 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:44,181 - src.utils - INFO - Generated 2 continuations for batch starting at 14. Generated example:  1 if the song likely belongs to the specified gen\n",
      "2025-05-28 00:27:44,182 - src.utils - INFO - Processing batch 16..18, batch_size=2\n",
      "2025-05-28 00:27:44,183 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"deathcore\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:44,220 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:48,132 - src.utils - INFO - Generated 2 continuations for batch starting at 16. Generated example:  song lyrics with genre for classification: 1\n",
      "\n",
      "```\n",
      "2025-05-28 00:27:48,132 - src.utils - INFO - Processing batch 18..20, batch_size=2\n",
      "2025-05-28 00:27:48,133 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"doom-metal\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:48,173 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:52,579 - src.utils - INFO - Generated 2 continuations for batch starting at 18. Generated example:  the lyrics: \"rics\": \"I scandalized my brother Whi\n",
      "2025-05-28 00:27:52,579 - src.utils - INFO - Processing batch 20..22, batch_size=2\n",
      "2025-05-28 00:27:52,580 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"drum&bass\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:52,626 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:27:56,789 - src.utils - INFO - Generated 2 continuations for batch starting at 20. Generated example:  (genre) \n",
      "\n",
      "0\n",
      "2025-05-28 00:27:56,790 - src.utils - INFO - Processing batch 22..24, batch_size=2\n",
      "2025-05-28 00:27:56,791 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"electro-pop\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:27:56,830 - src.utils - INFO - Obtained past_key_values from prefix run\n",
      "2025-05-28 00:28:00,467 - src.utils - INFO - Generated 2 continuations for batch starting at 22. Generated example:  the lyrics \"rics\": \"I scandalized my brother Whil\n",
      "2025-05-28 00:28:00,467 - src.utils - INFO - Processing batch 24..26, batch_size=2\n",
      "2025-05-28 00:28:00,468 - src.utils - INFO - Postfix sample: rics\": \"I scandalized my brother While admittin' that he sang some pretty songs (and he did) I'd heard that he'd been scandalizing me And, Lord, I knew that that was wrong (and I was) Now I'm lookin' at it over Something cool and feelin' fool enough to see What I had called my brother on Now he had every ri\",\n",
      "    \"genre\": \"electronica\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-05-28 00:28:00,508 - src.utils - INFO - Obtained past_key_values from prefix run\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Итерация по тестовому DataLoader\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Метод predict возвращает: predictions (np.ndarray), full_generated (List[str]), instruct_texts (List[str])\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     preds, full_outputs, instructs \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(batch, enable_thinking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m     all_predictions\u001b[38;5;241m.\u001b[39mappend(preds)\n\u001b[0;32m     21\u001b[0m     all_full_outputs\u001b[38;5;241m.\u001b[39mextend(full_outputs)\n",
      "Cell \u001b[1;32mIn[62], line 105\u001b[0m, in \u001b[0;36mZeroShotClassifierV1.predict\u001b[1;34m(self, batch, enable_thinking, debug)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[0;32m    103\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObtained past_key_values from prefix run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m cont \u001b[38;5;241m=\u001b[39m batch_generate_with_prefix_cache(\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m    108\u001b[0m     past,\n\u001b[0;32m    109\u001b[0m     postfix_ids,\n\u001b[0;32m    110\u001b[0m     postfix_mask,\n\u001b[0;32m    111\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    112\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m    113\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug\n\u001b[0;32m    114\u001b[0m )\n\u001b[0;32m    115\u001b[0m generated\u001b[38;5;241m.\u001b[39mextend(cont)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "Cell \u001b[1;32mIn[55], line 39\u001b[0m, in \u001b[0;36mbatch_generate_with_prefix_cache\u001b[1;34m(model, tokenizer, prefix_past_key_values, postfix_ids, postfix_mask, device, max_new_tokens, debug)\u001b[0m\n\u001b[0;32m     36\u001b[0m     full_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prefix_attention_mask, postfix_mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, prefix + postfix]\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m full_attention_mask\n\u001b[1;32m---> 39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n\u001b[0;32m     40\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]                \u001b[38;5;66;03m# [B, V]\u001b[39;00m\n\u001b[0;32m     41\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)       \u001b[38;5;66;03m# [B]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:850\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    846\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    849\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 850\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    851\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    852\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    853\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    854\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    855\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    856\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    857\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    858\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    859\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    861\u001b[0m )\n\u001b[0;32m    863\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:576\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    565\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[0;32m    566\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m         position_embeddings,\n\u001b[0;32m    574\u001b[0m     )\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    577\u001b[0m         hidden_states,\n\u001b[0;32m    578\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    579\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    580\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    581\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    582\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    583\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    584\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    586\u001b[0m     )\n\u001b[0;32m    588\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:304\u001b[0m, in \u001b[0;36mQwen3DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[0;32m    303\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m    305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    306\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Programms\\Anaconda\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:74\u001b[0m, in \u001b[0;36mQwen3RMSNorm.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m     72\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m     73\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 74\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     75\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = ZeroShotClassifierV1(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    genres=genres,\n",
    "    prompt_template=prompt_v1,\n",
    "    use_kv_cache=True,\n",
    "    device=\"cuda\",\n",
    "    max_lyrics_length=300,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "all_predictions = []\n",
    "all_full_outputs = []\n",
    "all_instruct_texts = []\n",
    "\n",
    "# Итерация по тестовому DataLoader\n",
    "for batch in test_loader:\n",
    "    # Метод predict возвращает: predictions (np.ndarray), full_generated (List[str]), instruct_texts (List[str])\n",
    "    preds, full_outputs, instructs = classifier.predict(batch, enable_thinking=False, debug=True)\n",
    "    all_predictions.append(preds)\n",
    "    all_full_outputs.extend(full_outputs)\n",
    "    all_instruct_texts.extend(instructs)\n",
    "\n",
    "# Конкатенация всех предсказаний\n",
    "all_predictions = np.vstack(all_predictions)  # shape: [num_samples, num_genres]\n",
    "\n",
    "# Выведем форму итогового массива и несколько примеров\n",
    "print(\"Predictions shape:\", all_predictions.shape)\n",
    "print(\"Example predictions (one-hot labels):\", all_predictions[:5])\n",
    "print(\"Example full outputs:\", all_full_outputs[:5])\n",
    "print(\"Example instruct texts:\", all_instruct_texts[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb6a87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_list = [value for _, value in val_dataset[1:3]['features']['lyrics'].items()]\n",
    "target_genre = [value[0] for _, value in val_dataset[1:3]['features']['genre_list'].items()]\n",
    "\n",
    "truncated_list = [text[:300] for text in lyrics_list]\n",
    "\n",
    "suffix_list = []\n",
    "for text, target_genre in zip(truncated_list, target_genre):\n",
    "    instruct = prompt_v1 % (text, target_genre)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruct}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "        do_sample=False\n",
    "    )\n",
    "    suffix = text[split_index:]\n",
    "    \n",
    "    suffix_list.append(suffix)\n",
    "\n",
    "# Suppose we already have prefix and split index\n",
    "prefix_list = [prefix for _ in range(2)]\n",
    "\n",
    "prefix_inputs = tokenizer(\n",
    "    prefix_list, # put here doubled prefixes\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prefix_outputs = model(\n",
    "        **prefix_inputs,\n",
    "        use_cache=True\n",
    "    )\n",
    "    prefix_past = prefix_outputs.past_key_values\n",
    "    \n",
    "generated_texts = batch_generate_with_prefix_cache(model, tokenizer, prefix_past, suffix_list)\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classifier with turned on thinking mode\n",
      "Ground trith labels: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Predicted labels: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "  1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1]]\n",
      "\n",
      "Let's take a look at specific instruct:\n",
      "<|im_start|>user\n",
      "You are a music genre expert. You will determine whether a song belongs to a specific genre based on its lyrics. You will be provided with a JSON input containing the lyrics and the target genre. Respond with 1 if the song likely belongs to the specified genre, and 0 if it does not.\n",
      "\n",
      "**Input format:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"Lyrics of the song\",\n",
      "    \"genre\": \"Target genre\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Output format:**\n",
      "```json\n",
      "{\n",
      "    \"predict\": 1  // if the song belongs to the genre\n",
      "    // or\n",
      "    \"predict\": 0  // if it does not\n",
      "}\n",
      "```\n",
      "\n",
      "**Lyrics with genre for classification:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"[Verse 1] Well, I'm standing here, freezing, outside your golden garden Uh got my ladder, leaned up against your wall Tonight's the night we planned to run away together Come on Dolly Mae, there's no time to stall But now you're telling me [Chorus] I\",\n",
      "    \"genre\": \"classical\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "And here is the answer:\n",
      " music genre expert\n",
      "<think>\n",
      "Okay, let's see. The user wants me to determine if a song is classified into a specific genre based on its lyrics. The input provided is a JSON with the lyrics and the target genre. The output should be 1 or 0 depending on whether the song is likely to belong to that genre.\n",
      "\n",
      "First, I need to look at the lyrics given. The example lyrics are from a song with the genre \"classical\". The user provided the lyrics in a JSON structure, and the genre is \"classical\". The task is to check if the lyrics are associated with classical music.\n",
      "\n",
      "Now, I should analyze the lyrics. The example lyrics mention \"outside your golden garden,\" which sounds like a garden, maybe a place associated with classical music. The mention of \"ladder\" and \"wall\" could be from a classical setting. The chorus talks about running away, which is a common theme in classical music, but I'm not sure. However, the key part here is the description of a garden and the ladder against a wall. These elements are more characteristic of classical music, especially in settings like a garden, which is often a setting for classical compositions.\n",
      "\n",
      "Since the lyrics do include elements that align with classical music, the prediction should be 1. The output JSON would have \"predict\": 1.\n",
      "</think>```json\n",
      "{\n",
      "    \"predict\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Actual genre was: ['jazz', 'pop', 'rock']\n",
      "\n",
      "Predicted genre is: ['alt-country', 'alt-rock', 'alternative', 'ambient', 'axé', 'black-metal', 'blues', 'bossa-nova', 'chillwave', 'classic-rock', 'classical', 'cloud-rap', 'country', 'dance', 'dancehall', 'death-metal', 'disco', 'doom-metal', 'dream-pop', 'drum&bass', 'dub', 'electro-pop', 'electronic', 'electronica', 'emo', 'emo-rap', 'folk', 'forró', 'funk', 'funk-carioca', 'garage-rock', 'gothic', 'grunge', 'hard-rock', 'hardcore', 'house', 'indie', 'indie-pop', 'indie-rock', 'j-pop', 'j-rock', 'jazz', 'jovem-guarda', 'k-pop', 'melodic-death-metal', 'mpb', 'new-wave', 'pagode', 'pop', 'pop-punk', 'pop-rock', 'post-punk', 'power-metal', 'power-pop', 'progressive-metal', 'progressive-rock', 'psychedelic', 'psychedelic-rock', 'punk', 'punk-rock', 'r&b', 'rap', 'reggae', 'reggaeton', 'rock', 'rockabilly', 'samba', 'screamo', 'sertanejo', 'shoegaze', 'soft-rock', 'soul', 'swing', 'synth-pop', 'thrash-metal', 'trap', 'tropical-house']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test classifier with turned off thinking mode\n",
      "Ground trith labels: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Predicted labels: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "  1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1]]\n",
      "\n",
      "Let's take a look at specific instruct:\n",
      "<|im_start|>user\n",
      "You are a music genre expert. You will determine whether a song belongs to a specific genre based on its lyrics. You will be provided with a JSON input containing the lyrics and the target genre. Respond with 1 if the song likely belongs to the specified genre, and 0 if it does not.\n",
      "\n",
      "**Input format:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"Lyrics of the song\",\n",
      "    \"genre\": \"Target genre\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Output format:**\n",
      "```json\n",
      "{\n",
      "    \"predict\": 1  // if the song belongs to the genre\n",
      "    // or\n",
      "    \"predict\": 0  // if it does not\n",
      "}\n",
      "```\n",
      "\n",
      "**Lyrics with genre for classification:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"[Verse 1] Well, I'm standing here, freezing, outside your golden garden Uh got my ladder, leaned up against your wall Tonight's the night we planned to run away together Come on Dolly Mae, there's no time to stall But now you're telling me [Chorus] I\",\n",
      "    \"genre\": \"classical\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "And here is the answer:\n",
      "```json\n",
      "{\n",
      "    \"predict\": 1\n",
      "}\n",
      "```\n",
      "\n",
      "Actual genre was: ['jazz', 'pop', 'rock']\n",
      "\n",
      "Predicted genre is: ['alt-country', 'alt-rock', 'alternative', 'ambient', 'axé', 'black-metal', 'blues', 'bossa-nova', 'chillwave', 'classic-rock', 'classical', 'cloud-rap', 'country', 'dance', 'dancehall', 'deathcore', 'disco', 'dream-pop', 'drum&bass', 'dub', 'electro-pop', 'electronic', 'electronica', 'emo', 'emo-rap', 'folk', 'forró', 'funk', 'funk-carioca', 'garage-rock', 'gothic', 'grunge', 'hard-rock', 'hardcore', 'heavy-metal', 'hip-hop', 'house', 'indie', 'indie-pop', 'indie-rock', 'j-pop', 'j-rock', 'jazz', 'jovem-guarda', 'k-pop', 'melodic-death-metal', 'metal', 'metalcore', 'mpb', 'new-wave', 'pagode', 'pop', 'pop-punk', 'pop-rock', 'post-hardcore', 'post-punk', 'power-metal', 'power-pop', 'progressive-metal', 'progressive-rock', 'psychedelic', 'psychedelic-rock', 'punk', 'punk-rock', 'r&b', 'rap', 'reggae', 'reggaeton', 'rock', 'rockabilly', 'samba', 'screamo', 'sertanejo', 'shoegaze', 'soft-rock', 'soul', 'swing', 'synth-pop', 'techno', 'trance', 'trap', 'trip-hop', 'tropical-house']\n"
     ]
    }
   ],
   "source": [
    "# def test_classifier(enable_thinking):\n",
    "#     classifier_v1 = ZeroShotClassifierV1(model, tokenizer, genres, prompt_v1, device=device, max_lyrics_length=250, batch_size=128)\n",
    "\n",
    "#     batch = next(iter(val_loader))\n",
    "#     batch['features'] = batch['features'][:1]\n",
    "#     ground_truth = batch['labels'][:1]\n",
    "\n",
    "#     predictions, generated_texts, instruct_texts = classifier_v1.predict(batch, enable_thinking=enable_thinking)\n",
    "#     print('Ground trith labels:', ground_truth)\n",
    "#     print('Predicted labels:', predictions)\n",
    "\n",
    "#     print(\"\\nLet's take a look at specific instruct:\")\n",
    "#     print(instruct_texts[10])\n",
    "#     print(\"\\nAnd here is the answer:\")\n",
    "#     print(generated_texts[10])\n",
    "#     print('\\nActual genre was:', one_hot_encoded_to_genre_list(ground_truth[0], idx2genre))\n",
    "#     print('\\nPredicted genre is:', one_hot_encoded_to_genre_list(predictions[0], idx2genre))\n",
    "    \n",
    "# print('Test classifier with turned on thinking mode')\n",
    "# test_classifier(True)\n",
    "\n",
    "# print('\\n\\n\\n\\nTest classifier with turned off thinking mode')\n",
    "# test_classifier(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
