{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554d005d",
   "metadata": {},
   "source": [
    "# Here we will use ptune learning method as baseline for genre classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a75c0ab-45b9-4742-98f9-98e058062b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T15:10:33.618011Z",
     "iopub.status.busy": "2025-05-25T15:10:33.617473Z",
     "iopub.status.idle": "2025-05-25T15:10:33.623775Z",
     "shell.execute_reply": "2025-05-25T15:10:33.622975Z",
     "shell.execute_reply.started": "2025-05-25T15:10:33.617989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import logging\n",
    "\n",
    "class DatasetTypes(enum.Enum):\n",
    "    whole = 0  # Returns all dataset\n",
    "    small = 1  # Returns only 1000 first rows from dataset\n",
    "\n",
    "class Params:\n",
    "    def __init__(self, exp_name='genre_classification', random_seed=1337, n_epoch=10, batch_size=8, dataset_type=DatasetTypes.whole, \n",
    "                 learning_rate=1e-4, weight_decay=1e-5):\n",
    "        self.random_seed = random_seed\n",
    "        self.exp_name = exp_name\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_type = dataset_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \", \".join(f\"{k}: {v}\" for k, v in vars(self).items())\n",
    "    \n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778119b3-d6f7-479f-a30f-5836a123396c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T15:10:35.318351Z",
     "iopub.status.busy": "2025-05-25T15:10:35.318076Z",
     "iopub.status.idle": "2025-05-25T15:10:35.337566Z",
     "shell.execute_reply": "2025-05-25T15:10:35.336833Z",
     "shell.execute_reply.started": "2025-05-25T15:10:35.318332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LyricsGenreDataset(Dataset):\n",
    "    def __init__(self, lyrics_list, features, labels, tokenizer, max_length=512):\n",
    "        self.lyrics = lyrics_list\n",
    "        self.features = features  # pandas df\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.lyrics[idx]\n",
    "        label = self.labels[idx]\n",
    "        features = self.features.iloc[idx].to_dict()  # <--- превращаем в словарь\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # [seq_len]\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # [seq_len]\n",
    "            'labels': torch.tensor(label, dtype=torch.float),  # [num_labels]\n",
    "            'features': features  # <--- теперь это dict, collate будет работать\n",
    "        }\n",
    "\n",
    "\n",
    "def one_hot_encoded_to_genre_list(predictions, idx2genre: dict = None):\n",
    "    ''' Predictions is array on n_genres size, where 1 if lyrics belongs to that genre and 0 if not'''    \n",
    "    genre_list = []\n",
    "    for i, value in enumerate(predictions):\n",
    "        if value == 1:\n",
    "            genre_list.append(idx2genre[i])\n",
    "    \n",
    "    return genre_list\n",
    "\n",
    "\n",
    "def get_datasets(df_path, tokenizer, dataset_type=DatasetTypes.whole, debug=False, train_size=0.7, test_size=0.15, val_size=0.15, random_seed=1337):\n",
    "    ''' Params:\n",
    "            df_path - path to .csv format file. Expected that it have 'lyrics' and 'genre' fields as base. Other fields will go to feature field of dataset\n",
    "        Returns:\n",
    "            dicts with torch datasets and some extra objects'''\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1.0\"\n",
    "    \n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df[:1000] if dataset_type == DatasetTypes.small else df  # For experiments use\n",
    "\n",
    "    if debug: \n",
    "        logger.info(str(df.head()))\n",
    "        \n",
    "    target = df['genre'].unique()\n",
    "    \n",
    "    if debug:\n",
    "        logger.info(str(sorted(target)))\n",
    "\n",
    "    all_genre_strings = df['genre'].unique()\n",
    "\n",
    "    # Разделяем по запятой и складываем в множество (чтобы получить только уникальные жанры)\n",
    "    all_genres = set()\n",
    "\n",
    "    for genre_string in all_genre_strings:\n",
    "        genres = genre_string.split(',')\n",
    "        all_genres.update(genres)\n",
    "\n",
    "    # Преобразуем в отсортированный список (по желанию)\n",
    "    all_genres_list = sorted(all_genres)\n",
    "\n",
    "    if debug:\n",
    "        logger.info(str(all_genres_list))\n",
    "        logger.info(str(f'Genres length: {len(all_genres_list)}'))\n",
    "        \n",
    "    df['genre_list'] = df['genre'].apply(lambda x: [g.strip() for g in x.split(',')])\n",
    "\n",
    "    # Используем MultiLabelBinarizer для преобразования в one-hot\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(df['genre_list'])\n",
    "    genres = mlb.classes_\n",
    "\n",
    "    if debug:\n",
    "        logger.info(f\"Unique genres number: {len(mlb.classes_)}\")\n",
    "        \n",
    "    idx2genre = {i: genre for i, genre in enumerate(genres)}\n",
    "    genre2idx = {genre: i for i, genre in enumerate(genres)}\n",
    "    \n",
    "    if debug:\n",
    "        genres_count = {genre_name: 0 for genre_name in genres}\n",
    "        for index, row in df.iterrows():\n",
    "            for genre in genres:\n",
    "                if genre in row['genre_list']:\n",
    "                    genres_count[genre] += 1\n",
    "\n",
    "        logger.info('Genres count')\n",
    "        for key, value in genres_count.items():\n",
    "            logger.info(f\"{key}: {value}\")\n",
    "\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "    df_shuffled['genre_list'] = df_shuffled['genre'].apply(lambda x: [g.strip() for g in x.split(',')])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(df_shuffled['genre_list'])\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        df_shuffled, y, test_size=test_size, random_state=random_seed\n",
    "    )\n",
    "\n",
    "    # Отношение валидационной выборки к оставшемуся (train + val)\n",
    "    val_ratio_of_temp = val_size / (train_size + val_size)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio_of_temp, random_state=random_seed\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        logger.info('Dataset sizes:')\n",
    "        logger.info('Train size:', len(X_train))\n",
    "        logger.info('Val size:', len(X_val))\n",
    "        logger.info('Test size:', len(X_test))\n",
    "        \n",
    "    train_dataset = LyricsGenreDataset(X_train['lyrics'].tolist(), X_train, y_train, tokenizer)\n",
    "    val_dataset = LyricsGenreDataset(X_val['lyrics'].tolist(), X_val, y_val, tokenizer)\n",
    "    test_dataset = LyricsGenreDataset(X_test['lyrics'].tolist(), X_test, y_test, tokenizer)\n",
    "    \n",
    "    return {\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        'genres': genres,\n",
    "        'idx2genre': idx2genre,\n",
    "        'genre2idx': genre2idx}\n",
    "    \n",
    "    \n",
    "def get_dataloaders(train_dataset: LyricsGenreDataset, val_dataset: LyricsGenreDataset, test_dataset: LyricsGenreDataset, batch_size):\n",
    "    # We eill use custion collate fn because we want features dict be in our dataset\n",
    "    def custom_collate_fn(batch):\n",
    "        batch_dict = default_collate([\n",
    "            {k: v for k, v in item.items() if k != 'features'}\n",
    "            for item in batch\n",
    "        ])\n",
    "\n",
    "        # Собираем features отдельно\n",
    "        if 'features' in batch[0]:\n",
    "            feature_dicts = [item['features'] for item in batch]\n",
    "            batch_dict['features'] = feature_dicts  # просто список словарей\n",
    "\n",
    "        return batch_dict\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cb3cd-8993-4c68-a133-ab55ce63daa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T15:10:40.477466Z",
     "iopub.status.busy": "2025-05-25T15:10:40.477185Z",
     "iopub.status.idle": "2025-05-25T15:10:40.485544Z",
     "shell.execute_reply": "2025-05-25T15:10:40.484837Z",
     "shell.execute_reply.started": "2025-05-25T15:10:40.477444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# ModelEvalInterface - interface for model evaluation. Model should get row of df as input and then return vector of predictions (prediction if row belongs to some class of not).\n",
    "class GenrePredictorInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def predict(self, batch_features: dict) -> np.array:\n",
    "        \"\"\"\n",
    "        Get batched input that contains 'input_ids', 'labels', 'features'\n",
    "        Returns prediction in binary format [batch_size, num_classes] as numpy array\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "def evaluate_model(model: GenrePredictorInterface, dataloader, device='cpu'):\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        targets = batch['labels']\n",
    "        preds = model.predict(batch)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_targets)\n",
    "\n",
    "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    # confusion_matrix — многоклассовая, тут нужна \"ошибочная матрица\" в multilabel стиле\n",
    "    # Мы сделаем aggregated confusion-like матрицу:\n",
    "    \n",
    "    num_classes = y_true.shape[1]\n",
    "    error_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        true_labels = np.where(y_true[i] == 1)[0]\n",
    "        pred_labels = np.where(y_pred[i] == 1)[0]\n",
    "\n",
    "        for pred in pred_labels:\n",
    "            for true in true_labels:\n",
    "                error_matrix[pred, true] += 1\n",
    "\n",
    "    metrics = {\n",
    "        'precision': prec,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'error_matrix': error_matrix\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adae4c37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T15:11:29.005803Z",
     "iopub.status.busy": "2025-05-25T15:11:29.005108Z",
     "iopub.status.idle": "2025-05-25T15:11:35.785944Z",
     "shell.execute_reply": "2025-05-25T15:11:35.785418Z",
     "shell.execute_reply.started": "2025-05-25T15:11:29.005780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c534801-2e02-4f0a-b96c-c488206b8bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T15:12:36.717018Z",
     "iopub.status.busy": "2025-05-25T15:12:36.715900Z",
     "iopub.status.idle": "2025-05-25T15:13:00.731483Z",
     "shell.execute_reply": "2025-05-25T15:13:00.730602Z",
     "shell.execute_reply.started": "2025-05-25T15:12:36.716983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3db2739dc84cf2b45c04c77655513b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57479e145a4bc1a016c597062b45a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e34bf2da3d42dba002732e409cf1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6957c6af2a42728c37a86c318d9e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093a61eb274b4bac80740638f3edb2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 15:12:41.193201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748185961.408085      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748185961.470340      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cedd0727034e69b64fb52a0ad77727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e55e5b220a4298b59cac0c653206cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simpliest model for demonstration scenario\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07da05e-48b2-420e-b638-586463dbfb97",
   "metadata": {},
   "source": [
    "## Get dataset with all genres and 1,294,054 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79a36b-e843-4653-8ee1-7997d6c8cf7c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path_to_csv = '../data/all_genres_downsampled.csv'\n",
    "data_dict = get_datasets(path_to_csv, tokenizer, dataset_type=DatasetTypes.small)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = data_dict['train_dataset'], data_dict['val_dataset'], data_dict['test_dataset']\n",
    "idx2genre, genre2idx = data_dict['idx2genre'], data_dict['genre2idx']\n",
    "genres = [key for key, _ in genre2idx.items()]\n",
    "\n",
    "batch_size = 16\n",
    "train_loader, val_loader, test_loader = get_dataloaders(train_dataset, val_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c066008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    get_peft_model,\n",
    "    PromptTuningInit\n",
    ")\n",
    "\n",
    "prompt_config = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text=f\"\"\"    You are an expert music genre classifier. Analyze these song lyrics features:\n",
    "    1. Themes (love, party, rebellion)\n",
    "    2. Language style (slang, poetic)\n",
    "    3. Rhythmic patterns\n",
    "    4. Cultural context\n",
    "    \n",
    "    Possible genres: {', '.join(genres)}\n",
    "    \n",
    "    Example 1:\n",
    "    Lyrics: \"We’re rolling down the street with the bass turned up, neon lights flashing...\"\n",
    "    Genre: Hip-Hop\n",
    "    \n",
    "    Example 2:\n",
    "    Lyrics: \"Sweet child o' mine, you're the only one that's on my mind\"\n",
    "    Genre: Rock\n",
    "    \n",
    "    Note: Be concise. Classify the following:\n",
    "    Lyrics: {{input_lyrics}}\n",
    "    Genre:\"\"\".replace(\"{\", \"{{\").replace(\"}\", \"}}\"),  # Подставляем реальные жанры из датасета\n",
    "    num_virtual_tokens=50,\n",
    "    tokenizer_name_or_path=model_name,\n",
    "    inference_mode=False\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, prompt_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.print_trainable_parameters()    # обучаются только soft-prompts\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ca19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_scheduler\n",
    "\n",
    "num_epochs = 100\n",
    "warmup_steps = 500\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Optimizer: only prompt embeddings are trainable\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(smoothing=0.1)\n",
    "\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\", optimizer=optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map genres to label token ids\n",
    "label_token_ids = {g: tokenizer.encode(' ' + g, add_special_tokens=False)[0] for g in idx2genre.values()}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # genre indices\n",
    "\n",
    "        # Build prompt: virtual tokens + input text\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids,  # dummy for LM\n",
    "            return_dict=True\n",
    "        )\n",
    "        lm_logits = outputs.logits  # [batch, seq_len, vocab]\n",
    "\n",
    "        # Use last token logits to predict genre token\n",
    "        last_logit = lm_logits[:, -1, :]  # [batch, vocab]\n",
    "        target_ids = torch.tensor(\n",
    "            [label_token_ids[idx2genre[i.item()]] for i in labels],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(last_logit, target_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            target_ids = torch.tensor(\n",
    "                [label_token_ids[idx2genre[i.item()]] for i in labels],\n",
    "                device=device\n",
    "            )\n",
    "            correct += (preds == target_ids).sum().item()\n",
    "            total += labels.size(0)\n",
    "            torch.cuda.empty_cache()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Val accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_token_ids = {g: tokenizer.encode(' ' + g, add_special_tokens=False)[0] for g in idx2genre.values()}\n",
    "id2label_token = {v: k for k, v in label_token_ids.items()}\n",
    "\n",
    "model.eval()\n",
    "print(\"\\nTesting on test set:\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        for i in range(len(labels)):\n",
    "            pred_token = preds[i].item()\n",
    "            true_label = idx2genre[labels[i].item()]\n",
    "            pred_label = id2label_token.get(pred_token, 'UNKNOWN')\n",
    "            print(f\"Predicted genre is: {pred_label:<15} | Actual genre is: {true_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7511036,
     "sourceId": 11947470,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
