{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f637fa2",
   "metadata": {},
   "source": [
    "## Here we will use zero-shot learning method as baseline for genre classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c89bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94e9aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from src.utils import logger, DatasetTypes\n",
    "from src.data import get_datasets, get_dataloaders, one_hot_encoded_to_genre_list\n",
    "from src.metrics import GenrePredictorInterface, evaluate_model\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd3494",
   "metadata": {},
   "source": [
    "## Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e98cc299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af248adb8664366ac1f4e8c70e1ecda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programms\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zaxar\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-0.6B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96c33a383d141a3b02fdab0234e7899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725e7b12f4454586a5ec1011fcdc952d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-05-24 14:12:20,461 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cd0244ba2d40b2b6695172d41dd0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e03bab6fa2e483d8217a2ce97030b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-05-24 14:12:24,289 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e557a28c1d84117b6ee221867a771c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee37b97650542faa7b2c10859e30ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simpliest model for demonstration scenario\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d5c27",
   "metadata": {},
   "source": [
    "## Get dataset with all genres and 1,294,054 examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a459fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = '../data/all_genres_downsampled.csv'\n",
    "data_dict = get_datasets(path_to_csv, tokenizer, dataset_type=DatasetTypes.small)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = data_dict['train_dataset'], data_dict['val_dataset'], data_dict['test_dataset']\n",
    "idx2genre, genre2idx = data_dict['idx2genre'], data_dict['genre2idx']\n",
    "genres = [key for key, _ in genre2idx.items()]\n",
    "\n",
    "batch_size = 16\n",
    "traid_loader, val_loader, test_loader = get_dataloaders(train_dataset, val_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814c30d",
   "metadata": {},
   "source": [
    "## Sobstvenno, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5f44f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v1 = '''You are a music genre expert. You will determine whether a song belongs to a specific genre based on its lyrics. You will be provided with a JSON input containing the lyrics and the target genre. Respond with 1 if the song likely belongs to the specified genre, and 0 if it does not.\n",
    "\n",
    "**Input format:**\n",
    "```json\n",
    "{\n",
    "    \"lyrics\": \"Lyrics of the song\",\n",
    "    \"genre\": \"Target genre\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Output format:**\n",
    "```json\n",
    "{\n",
    "    \"predict\": 1  // if the song belongs to the genre\n",
    "    // or\n",
    "    \"predict\": 0  // if it does not\n",
    "}\n",
    "```\n",
    "\n",
    "**Lyrics with genre for classification:**\n",
    "```json\n",
    "{\n",
    "    \"lyrics\": \"%s\",\n",
    "    \"genre\": \"%s\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Your output**:\n",
    "'''\n",
    "\n",
    "def parse_model_response(response: str) -> int:\n",
    "    try:\n",
    "        # Попробуем извлечь JSON через регулярку (на случай мусора вокруг)\n",
    "        match = re.search(r'\\{[^}]*\"predict\"\\s*:\\s*(0|1)[^}]*\\}', response)\n",
    "        if match:\n",
    "            data = json.loads(match.group(0))\n",
    "            return int(data['predict'])\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "\n",
    "    raise ValueError(\"Could not parse prediction from model response.\")\n",
    "\n",
    "\n",
    "class ZeroShotClassifier(GenrePredictorInterface):\n",
    "    def __init__(self, model, tokenizer, genres, prompt_template, device=\"cuda\", max_lyrics_length=300):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.genres = genres  # список всех возможных жанров\n",
    "        self.device = device\n",
    "        self.max_lyrics_length = max_lyrics_length\n",
    "        self.prompt_template = prompt_template\n",
    "        \n",
    "    def _make_prompts(self, lyrics: str) -> list[str]:\n",
    "        truncated = lyrics[:self.max_lyrics_length].replace('\\n', ' ').replace('\"', \"'\")\n",
    "        prompts = [self.prompt_template % (truncated, genre) for genre in self.genres]\n",
    "        return prompts\n",
    "\n",
    "    def _parse_response(self, response: str) -> int:\n",
    "        try:\n",
    "            match = re.search(r'\\{[^}]*\"predict\"\\s*:\\s*(0|1)[^}]*\\}', response)\n",
    "            if match:\n",
    "                data = json.loads(match.group(0))\n",
    "                return int(data[\"predict\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\")\n",
    "        return 0  # fallback to 0 if anything goes wrong\n",
    "    \n",
    "\n",
    "def make_prompts(lyrics: str, genres) -> list[str]:\n",
    "    truncated = lyrics[:300].replace('\\n', ' ').replace('\"', \"'\")\n",
    "    prompts = [prompt_v1 % (truncated, genre) for genre in genres]\n",
    "    return prompts\n",
    "\n",
    "def parse_response(response: str) -> int:\n",
    "    try:\n",
    "        match = re.search(r'\\{[^}]*\"predict\"\\s*:\\s*(0|1)[^}]*\\}', response)\n",
    "        if match:\n",
    "            data = json.loads(match.group(0))\n",
    "            return int(data[\"predict\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Parse error: {e}\")\n",
    "    return 0  # fallback to 0 if anything goes wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13c477",
   "metadata": {},
   "source": [
    "Main mechanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1] Well, I'm standing here, freezing, outside your golden garden Uh got my ladder, leaned up \n",
      "jazz\n",
      "You are a music genre expert. You will determine whether a song belongs to a specific genre based on its lyrics. You will be provided with a JSON input containing the lyrics and the target genre. Respond with 1 if the song likely belongs to the specified genre, and 0 if it does not.\n",
      "\n",
      "**Input format:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"Lyrics of the song\",\n",
      "    \"genre\": \"Target genre\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Output format:**\n",
      "```json\n",
      "{\n",
      "    \"predict\": 1  // if the song belongs to the genre\n",
      "    // or\n",
      "    \"predict\": 0  // if it does not\n",
      "}\n",
      "```\n",
      "\n",
      "**Lyrics with genre for classification:**\n",
      "```json\n",
      "{\n",
      "    \"lyrics\": \"[Verse 1] Well, I'm standing here, freezing, outside your golden garden Uh got my ladder, leaned up against your wall Tonight's the night we planned to run away together Come on Dolly Mae, there's no time to stall But now you're telling me [Chorus] I think I better wait until tomorrow I think I bett\",\n",
      "    \"genre\": \"jazz\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Your output**:\n",
      "\n",
      "thinking content: \n",
      "content: ```json\n",
      "{\n",
      "    \"predict\": 1\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# suppose we have lyrics and target genre\n",
    "lyrics = val_dataset[0]['features']['lyrics']\n",
    "target_genre = val_dataset[0]['features']['genre_list'][0]\n",
    "print(lyrics[:100])\n",
    "print(target_genre)\n",
    "\n",
    "# let's try ask model if that song is belongs to target genre\n",
    "truncated = lyrics[:300]\n",
    "instruct = prompt_v1 % (truncated, target_genre)\n",
    "\n",
    "print(instruct)\n",
    "\n",
    "# prepare the model input\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": instruct}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    "    do_sample=False\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "try:\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "    \n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f18076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotClassifierV1(ZeroShotClassifier):\n",
    "    def __init__(self, model, tokenizer, genres, prompt_template, device=\"cuda\", max_lyrics_length=300, batch_size=2):\n",
    "        \"\"\"\n",
    "        batch_size — размер мини-батча для генерации (не batch['features']).\n",
    "        \"\"\"\n",
    "        super().__init__(model, tokenizer, genres, prompt_template, device, max_lyrics_length)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def predict(self, batch: dict, enable_thinking=False, debug: bool = False) -> np.ndarray:\n",
    "        lyrics_list = [row['lyrics'] for row in batch['features']]\n",
    "        all_prompts = []\n",
    "        index_map = []\n",
    "\n",
    "        for i, lyrics in enumerate(lyrics_list):\n",
    "            truncated = lyrics[:self.max_lyrics_length]\n",
    "            for genre in self.genres:\n",
    "                prompt = self.prompt_template % (truncated, genre)\n",
    "                all_prompts.append(prompt)\n",
    "                index_map.append(i)\n",
    "\n",
    "        if debug:\n",
    "            logger.info(f\"Total prompts: {len(all_prompts)}\")\n",
    "            logger.info(f\"Example prompt:\\n{all_prompts[0]}\")\n",
    "\n",
    "        # Шаблоны превращаются в текст через chat_template\n",
    "        instruct_texts = [\n",
    "            self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": prompt}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=enable_thinking,\n",
    "                do_sample=False\n",
    "            )\n",
    "            for prompt in all_prompts\n",
    "        ]\n",
    "\n",
    "        # Подаём списками по batch_size\n",
    "        generated_texts = []\n",
    "        full_generated = []\n",
    "        self.model.eval()\n",
    "\n",
    "        for i in range(0, len(instruct_texts), self.batch_size):\n",
    "            batch_texts = instruct_texts[i:i + self.batch_size]\n",
    "            model_inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **model_inputs,\n",
    "                    max_new_tokens=1024,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            for j in range(len(batch_texts)):\n",
    "                output_ids = outputs[j][len(model_inputs[\"input_ids\"][j]):].tolist()\n",
    "\n",
    "                try:\n",
    "                    split_idx = len(output_ids) - output_ids[::-1].index(151668)\n",
    "                except ValueError:\n",
    "                    split_idx = 0\n",
    "\n",
    "                thinking_content = tokenizer.decode(output_ids[:split_idx], skip_special_tokens=True).strip(\"\\n\")\n",
    "                main_output = self.tokenizer.decode(output_ids[split_idx:], skip_special_tokens=True).strip()\n",
    "                \n",
    "                full_generated.append(thinking_content + main_output)\n",
    "                generated_texts.append(main_output)\n",
    "\n",
    "        if debug:\n",
    "            logger.info(\"Sample model outputs:\\n\" + \"\\n---\\n\".join(generated_texts[:3]))\n",
    "\n",
    "        # Собираем финальные предсказания\n",
    "        batch_size = len(lyrics_list)\n",
    "        num_genres = len(self.genres)\n",
    "        predictions = np.zeros((batch_size, num_genres), dtype=np.int32)\n",
    "\n",
    "        for i, raw_output in enumerate(generated_texts):\n",
    "            sample_idx = index_map[i]\n",
    "            genre_idx = i % num_genres\n",
    "            try:\n",
    "                predictions[sample_idx, genre_idx] = self._parse_response(raw_output)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to parse output: {raw_output}, error: {e}\")\n",
    "\n",
    "        if debug:\n",
    "            for i, pred in enumerate(predictions):\n",
    "                predicted_genres = [g for g, flag in zip(self.genres, pred) if flag]\n",
    "                logger.info(f\"Sample {i} predicted genres: {predicted_genres}\")\n",
    "\n",
    "        return predictions, full_generated, instruct_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classifier with turned on thinking mode\n"
     ]
    }
   ],
   "source": [
    "def test_classifier(enable_thinking):\n",
    "    classifier_v1 = ZeroShotClassifierV1(model, tokenizer, genres, prompt_v1, device=device, max_lyrics_length=250, batch_size=128)\n",
    "\n",
    "    batch = next(iter(val_loader))\n",
    "    batch['features'] = batch['features'][:1]\n",
    "    ground_truth = batch['labels'][:1]\n",
    "\n",
    "    predictions, generated_texts, instruct_texts = classifier_v1.predict(batch, enable_thinking=enable_thinking)\n",
    "    print('Ground trith labels:', ground_truth)\n",
    "    print('Predicted labels:', predictions)\n",
    "\n",
    "    print(\"\\nLet's take a look at specific instruct:\")\n",
    "    print(instruct_texts[10])\n",
    "    print(\"\\nAnd here is the answer:\")\n",
    "    print(generated_texts[10])\n",
    "    print('\\nActual genre was:', one_hot_encoded_to_genre_list(ground_truth[0], idx2genre))\n",
    "    print('\\nPredicted genre is:', one_hot_encoded_to_genre_list(predictions[0], idx2genre))\n",
    "    \n",
    "print('Test classifier with turned on thinking mode')\n",
    "test_classifier(True)\n",
    "\n",
    "print('\\n\\n\\n\\nTest classifier with turned off thinking mode')\n",
    "test_classifier(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
